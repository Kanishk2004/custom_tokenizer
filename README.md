# Custom Word-Level Tokenizer

A comprehensive, educational web application implementing a word-level tokenizer built in pure JavaScript. This project demonstrates the fundamental concepts of tokenization, vocabulary management, and text encoding/decoding as used in Large Language Models (LLMs).

![Tokenizer Demo](https://img.shields.io/badge/Demo-Live-green) ![JavaScript](https://img.shields.io/badge/JavaScript-ES6+-yellow) ![File-Based](https://img.shields.io/badge/Storage-File--Based-blue) ![License](https://img.shields.io/badge/License-MIT-red)

## ğŸ¯ Project Overview

This tokenizer is designed to help developers understand the basics of how text is processed in machine learning models. It features a clean, file-based architecture that persists vocabulary between sessions, making it suitable for both learning and practical applications.

## âš¡ Quick Start

Get up and running in seconds:

```bash
npm start
# Open http://localhost:3001 in your browser
```

That's it! The web interface will open where you can immediately start tokenizing text.

### Key Features

- âœ… **Pure JavaScript Implementation** - No external dependencies
- ğŸ—‚ï¸ **File-Based Vocabulary Storage** - Persistent vocabulary that survives restarts
- ğŸ”„ **Dynamic Vocabulary Expansion** - Automatically adds new tokens during encoding
- ğŸ“ **Case-Insensitive Processing** - Normalizes text to lowercase for better vocabulary efficiency
- ğŸ¨ **Modern Web Interface** - Beautiful, responsive frontend for interactive testing
- ğŸ”§ **Educational Focus** - Well-commented code for learning purposes
- âš¡ **Real-time Processing** - Instant tokenization and encoding feedback

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage Examples](#usage-examples)
- [API Reference](#api-reference)
- [Web Interface](#web-interface)
- [Architecture](#architecture)
- [File Structure](#file-structure)
- [Contributing](#contributing)
- [License](#license)

## ğŸš€ Installation

### Prerequisites

- **Node.js** (version 14 or higher)
- **Web browser** (for web interface)
- **Git** (optional, for cloning)

### Setup Instructions

1. **Clone the repository** (or download as ZIP):
   ```bash
   git clone <your-repo-url>
   cd custom-word-level-tokenizer
   ```

2. **No additional installation required!** This project uses only vanilla JavaScript and built-in Node.js modules.

## ğŸƒâ€â™‚ï¸ Quick Start

### Starting the Application

```bash
# Start the web server
npm start

# Alternative: development mode (same as start)
npm run dev
```

### Using the Web Interface

1. **Start the application**:
   ```bash
   npm start
   ```

2. **Open your browser** to the URL shown in the terminal (typically `http://localhost:3001`)

3. **Start tokenizing!** Enter text and see real-time results in the beautiful web interface.

## ğŸ’¡ Usage Examples

### Web Interface Usage

The primary way to use this tokenizer is through the interactive web interface:

1. **Enter text** in the input area
2. **Click "Tokenize"** to see the text split into tokens
3. **Click "Encode"** to convert tokens to numerical IDs
4. **Click "Decode"** to convert the IDs back to text
5. **Manage vocabulary** using the vocabulary management buttons

### Programmatic Usage

You can also use the tokenizer programmatically:

```javascript
const { Tokenizer } = require('./index.js');

// Create a new tokenizer instance
const tokenizer = new Tokenizer();

// Tokenize text into words and punctuation
const text = "Hello, world! This is a test.";
const tokens = tokenizer.tokenize(text);
console.log(tokens);
// Output: ['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']
```

### High-Level API Usage

```javascript
const { TokenizerAPI } = require('./index.js');

// Create API instance
const api = new TokenizerAPI();

// Process text with automatic vocabulary building
const result = await api.processText("Hello, world!");
console.log(result);
// Output: { tokens: [...], encodedIds: [...], vocabSize: 8 }
```

### Building Vocabulary

```javascript
// Build vocabulary from training texts
const trainingTexts = [
    "Hello, world! This is a simple tokenizer.",
    "It can handle punctuation and words.",
    "Machine learning is fascinating!"
];

const vocab = tokenizer.buildVocab(trainingTexts);
console.log('Vocabulary size:', tokenizer.getVocabSize());
```

### Encoding and Decoding

```javascript
// Encode text to token IDs
const text = "Hello, machine learning!";
const encoded = tokenizer.encode(text, true); // true = expand vocabulary
console.log('Encoded:', encoded);

// Decode back to text
const decoded = tokenizer.decode(encoded);
console.log('Decoded:', decoded);
```

### Working with Unknown Tokens

```javascript
// Encoding without vocabulary expansion
const unknownText = "This contains completely_new_words.";
const encoded = tokenizer.encode(unknownText, false); // false = no expansion
console.log('Encoded with UNK tokens:', encoded);

const decoded = tokenizer.decode(encoded);
console.log('Decoded:', decoded);
// Output: "this contains [UNK] [UNK]."
```

## ğŸ“š API Reference

### Core Functions

#### `tokenize(text)`
Converts text into an array of tokens (words and punctuation).

- **Parameters**: `text` (string) - Input text to tokenize
- **Returns**: `string[]` - Array of lowercase tokens
- **Example**:
  ```javascript
  tokenize("Hello, World!") // ['hello', ',', 'world', '!']
  ```

#### `buildVocab(texts, filename?)`
Builds vocabulary from an array of training texts.

- **Parameters**: 
  - `texts` (string[]) - Array of training texts
  - `filename` (string, optional) - Vocabulary file path (default: 'vocab.json')
- **Returns**: `Object` - Vocabulary mapping (token â†’ ID)

#### `encode(text, expandVocab?, filename?)`
Encodes text into an array of token IDs.

- **Parameters**:
  - `text` (string) - Text to encode
  - `expandVocab` (boolean, default: true) - Whether to add new tokens to vocabulary
  - `filename` (string, optional) - Vocabulary file path
- **Returns**: `number[]` - Array of token IDs

#### `decode(ids, filename?)`
Decodes array of token IDs back to text.

- **Parameters**:
  - `ids` (number[]) - Array of token IDs
  - `filename` (string, optional) - Vocabulary file path
- **Returns**: `string` - Decoded text

### Utility Functions

#### `getVocabSize(filename?)`
Returns the current vocabulary size.

#### `printVocab(filename?)`
Prints the vocabulary to console for debugging.

#### `saveVocabToFile(filename?)`
Manually saves vocabulary to file.

#### `loadVocabFromFile(filename?)`
Loads vocabulary from file.

## ğŸ¨ Web Interface

The web interface provides an intuitive way to test the tokenizer:

### Features

- **ğŸ“ Text Input Area** - Enter text for tokenization
- **ğŸ”§ Control Panel** - Buttons for encode, decode, and clear operations
- **ğŸ“Š Live Statistics** - Real-time vocabulary size and processing stats
- **ğŸ“‹ Results Display** - Visual representation of tokens, encoded IDs, and decoded text
- **ğŸ—‚ï¸ Vocabulary Management** - Load, save, reset, and view vocabulary
- **ğŸ“° Activity Log** - Track all operations and changes
- **ğŸ“± Responsive Design** - Works on desktop and mobile devices

### Interface Sections

1. **Header** - Project title and status indicators
2. **Statistics Cards** - Key metrics (vocab size, tokens processed, etc.)
3. **Input Panel** - Text area and control buttons
4. **Results Panel** - Tokenization and encoding results
5. **Vocabulary Display** - Current vocabulary table
6. **Activity Log** - Operation history

## ğŸ—ï¸ Architecture

### Design Principles

- **File-First Approach** - Vocabulary stored in JSON files
- **Stateless Functions** - No global state, all data from files
- **Atomic Operations** - Each operation is independent and safe
- **Educational Focus** - Code is readable and well-documented

### Data Flow

```
Text Input â†’ Tokenization â†’ Vocabulary Lookup â†’ Encoding â†’ File Storage
     â†“
File Storage â†’ Vocabulary Load â†’ Decoding â†’ Text Output
```

### File-Based Storage

- **vocab.json** - Primary vocabulary file
- **Automatic Persistence** - Changes saved immediately
- **JSON Format** - Human-readable and debuggable
- **Metadata Included** - Timestamps and version information

## ğŸ“ File Structure

```
01_customTokeniser/
â”œâ”€â”€ src/                  # Core source code
â”‚   â”œâ”€â”€ tokenizer.js      # Main Tokenizer class
â”‚   â”œâ”€â”€ api.js            # High-level API wrapper
â”‚   â””â”€â”€ utils.js          # Utility functions
â”œâ”€â”€ public/               # Web application
â”‚   â”œâ”€â”€ index.html        # Beautiful web interface
â”‚   â”œâ”€â”€ frontend.js       # Frontend JavaScript logic
â”‚   â””â”€â”€ server.js         # Web server
â”œâ”€â”€ data/                 # Data storage
â”‚   â””â”€â”€ vocabulary.json   # Auto-generated vocabulary
â”œâ”€â”€ index.js              # Main entry point (exports all classes)
â”œâ”€â”€ package.json          # Project configuration
â”œâ”€â”€ README.md             # This documentation
â””â”€â”€ .gitignore            # Git ignore patterns
```

### File Descriptions

- **`src/tokenizer.js`** - Core Tokenizer class with clean implementation
- **`src/api.js`** - High-level TokenizerAPI for simplified usage
- **`src/utils.js`** - TokenizerUtils with helper functions
- **`public/index.html`** - Modern, responsive web interface
- **`public/frontend.js`** - Web interface logic and UI interactions
- **`public/server.js`** - Web server for the application
- **`data/vocabulary.json`** - Auto-generated vocabulary storage
- **`index.js`** - Main entry point that exports all classes

## ğŸ”§ Advanced Features

### Custom Vocabulary Management

```javascript
// Load custom vocabulary
tokenizer.loadVocabFromFile('custom_vocab.json');

// Save current vocabulary
tokenizer.saveVocabToFile('backup_vocab.json');

// Get vocabulary statistics
console.log('Size:', tokenizer.getVocabSize());
console.log('Vocab:', tokenizer.getVocab());
```

### Batch Processing

```javascript
// Process multiple texts
const texts = [
    "First document to process",
    "Second document with different content",
    "Third document with more text"
];

texts.forEach((text, index) => {
    const encoded = tokenizer.encode(text);
    console.log(`Document ${index + 1}:`, encoded);
});
```

### Integration with Other Systems

```javascript
// Use in Express.js API
app.post('/tokenize', (req, res) => {
    const { text } = req.body;
    const tokens = tokenizer.tokenize(text);
    const encoded = tokenizer.encode(text);
    
    res.json({
        original: text,
        tokens: tokens,
        encoded: encoded,
        vocabSize: tokenizer.getVocabSize()
    });
});
```

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

### Development Setup

1. **Fork the repository**
2. **Create a feature branch**:
   ```bash
   git checkout -b feature/your-feature-name
   ```
3. **Make your changes**
4. **Test thoroughly**:
   ```bash
   node index.js  # Test core functionality
   # Open index.html in browser to test frontend
   ```
5. **Submit a pull request**

### Contribution Guidelines

- **Code Style** - Use consistent formatting and meaningful names
- **Documentation** - Update README and code comments
- **Testing** - Ensure all features work as expected
- **Compatibility** - Maintain Node.js 14+ compatibility

### Ideas for Contributions

- ğŸš€ **Performance Optimizations** - Faster tokenization algorithms
- ğŸ¨ **UI Improvements** - Better frontend design and UX
- ğŸ“Š **Analytics** - Token frequency analysis and statistics
- ğŸ”§ **Additional Features** - Subword tokenization, custom rules
- ğŸ“± **Mobile App** - React Native or Flutter implementation

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Inspiration** - Modern tokenizers like GPT's BPE and SentencePiece
- **Educational Resources** - Various NLP and machine learning courses
- **Community** - Feedback and suggestions from developers

## ğŸ“ Support

- **Issues** - Report bugs via GitHub Issues
- **Questions** - Ask questions in GitHub Discussions
- **Documentation** - Check this README and code comments

---

**Built with â¤ï¸ for the developer community to understand tokenization in LLMs**
